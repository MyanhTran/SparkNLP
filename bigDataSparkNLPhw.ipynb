{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXM01lgs-kfh"
      },
      "source": [
        "Question 2: Spark NLP\n",
        "\n",
        "In this task you are required to import SparkNLP library to implement a text classification model.\n",
        "Use AGNews dataset (check reference 1 about how you can download it) to train for 5 epochs and compare\n",
        "the performance of these different models:\n",
        "\n",
        "a) Use BERT embeddings with a generic annotator model in SparkNLP called ClassifierDL, without\n",
        "any text preprocessing steps and find the test accuracy for it.\n",
        "\n",
        "b) Add preprocessing steps, specifically lemmatization and stop word removal, to the pipeline in (a)\n",
        "and compare its impact on the overall performance of the model. Report the test accuracies when\n",
        "each step is implemented individually and when they are used together. Identify the pipeline that\n",
        "yields the highest test accuracy and give a brief explanation of why it performs the best.\n",
        "\n",
        "c) Lastly, select the best pipeline from (a) and (b) and use RoBerta embeddings instead of BERT\n",
        "embeddings. Report which embedding gives the best results and why.\n",
        "\n",
        "You can use Google Colab to do this task. Use the following links for reference:\n",
        "1) https://github.com/JohnSnowLabs/spark-nlpworkshop/blob/master/tutorials/Certification_Trainings/Public/5.1_Text_classification_example\n",
        "s_in_SparkML_SparkNLP.ipynb\n",
        "2) https://towardsdatascience.com/text-classification-in-spark-nlp-with-bert-and-universalsentence-encoders-e644d618ca32\n",
        "3) https://nlp.johnsnowlabs.com/docs/en/quickstart\n",
        "4) https://github.com/JohnSnowLabs/spark-nlpworkshop/tree/master/tutorials/Certification_Trainings/Public"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iumTJQxaoh5x"
      },
      "source": [
        "#Set up SparkNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h93kAwN_cY1"
      },
      "outputs": [],
      "source": [
        "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.3.0 -s 4.3.2 -g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VF7XqW6q-b6v"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "spark = sparknlp.start(gpu = True)# for GPU training >> sparknlp.start(gpu = True)\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpjvoNuFob7M"
      },
      "source": [
        "#Import data and set up train, test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZwzCFWHCFfd"
      },
      "outputs": [],
      "source": [
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv\n",
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsHsPvjTEa8C"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "trainDataset = spark.read \\\n",
        "      .option(\"header\", True) \\\n",
        "      .csv(\"news_category_train.csv\")\n",
        "\n",
        "trainDataset.groupBy(\"category\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .show()\n",
        "\n",
        "testDataset = spark.read \\\n",
        "      .option(\"header\", True) \\\n",
        "      .csv(\"news_category_test.csv\")\n",
        "\n",
        "\n",
        "testDataset.groupBy(\"category\") \\\n",
        "      .count() \\\n",
        "      .orderBy(col(\"count\").desc()) \\\n",
        "      .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6MBCCeE5VK6"
      },
      "source": [
        "#a, Use BERT embeddings and ClassifiedDL on raw data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTZNpFk39VGx"
      },
      "source": [
        "install BERT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BoMISIR6lmT"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers==4.15.0 tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IyY94br-KPC"
      },
      "outputs": [],
      "source": [
        "#start sparkNLP by Document Assembler call\n",
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"description\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "bert_sent = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "# the classes/labels/categories are in category column\n",
        "classsifierdl = ClassifierDLApproach()\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"class\")\\\n",
        "    .setLabelColumn(\"category\")\\\n",
        "    .setMaxEpochs(5)\\\n",
        "    .setEnableOutputLogs(True)\\\n",
        "    .setLr(0.001)\\\n",
        "    .setBatchSize(1)\n",
        "\n",
        "bert_sent_clf_pipeline = Pipeline(stages = [document,\n",
        "                                            bert_sent,\n",
        "                                            classsifierdl])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X1XpbWc--zn"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "bert_sent_pipelineModel = bert_sent_clf_pipeline.fit(trainDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06BtxJQR-eej"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "preds = bert_sent_pipelineModel.transform(testDataset)\n",
        "\n",
        "preds_df = preds.select('category','description',\"class.result\").toPandas()\n",
        "\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "\n",
        "print (classification_report(preds_df['category'], preds_df['result']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEhT0REtbeqr"
      },
      "source": [
        "BERT embeddings and generic annotator ClassifierDL gives accuracy of 89% without any pre-processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2z37eXBbxH8"
      },
      "source": [
        "#b, add pre-processing (lemmatization and stop-word removal) to a) individually and together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHPHq2qZdyHa"
      },
      "source": [
        "Set up pre-process of tokenize, etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9W5J-oBd36x"
      },
      "source": [
        "* Add stop word removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3pWjnH7b_eb"
      },
      "outputs": [],
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"description\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "    .setInputCols(\"token\")\\\n",
        "    .setOutputCol(\"cleanTokens\")\\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "stop_word_pipeline = Pipeline(stages = [document,\n",
        "                                        tokenizer,\n",
        "                                        stopwords_cleaner,\n",
        "                                        bert_sent,\n",
        "                                        classsifierdl])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxTBhvIjfQAJ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "stop_word_pipeline_Model = stop_word_pipeline.fit(trainDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooY-sh7JfV_5"
      },
      "outputs": [],
      "source": [
        "preds = stop_word_pipeline_Model.transform(testDataset)\n",
        "\n",
        "preds_df = preds.select('category','description',\"class.result\").toPandas()\n",
        "\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "\n",
        "print (classification_report(preds_df['category'], preds_df['result']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZFjhGpUioqV"
      },
      "source": [
        "* Add lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLVTK3ZpfSi1"
      },
      "outputs": [],
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"description\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"lemma\")\n",
        "\n",
        "bert_sent = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "classsifierdl = ClassifierDLApproach()\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"class\")\\\n",
        "    .setLabelColumn(\"category\")\\\n",
        "    .setMaxEpochs(5)\\\n",
        "    .setEnableOutputLogs(True)\\\n",
        "    .setLr(0.001)\n",
        "\n",
        "lemma_pipeline = Pipeline(stages = [documentAssembler,\n",
        "                                    tokenizer,\n",
        "                                    lemma,\n",
        "                                    bert_sent,\n",
        "                                    classsifierdl])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCY4-fPPjS6K"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "lemma_pipeline_Model = lemma_pipeline.fit(trainDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnR4e29ojcSA"
      },
      "outputs": [],
      "source": [
        "preds = lemma_pipeline_Model.transform(testDataset)\n",
        "\n",
        "preds_df = preds.select('category','description',\"class.result\").toPandas()\n",
        "\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "\n",
        "print (classification_report(preds_df['category'], preds_df['result']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvEiuSOrjf_N"
      },
      "source": [
        "* Add both stope-word removal and lemma to BERT pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-BdqXWijl53"
      },
      "outputs": [],
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"description\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "    .setInputCols(\"token\")\\\n",
        "    .setOutputCol(\"cleanTokens\")\\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"lemma\")\n",
        "\n",
        "bert_sent = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "classsifierdl = ClassifierDLApproach()\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"class\")\\\n",
        "    .setLabelColumn(\"category\")\\\n",
        "    .setMaxEpochs(5)\\\n",
        "    .setEnableOutputLogs(True)\\\n",
        "    .setLr(0.001)\n",
        "\n",
        "combined_pipeline = Pipeline(stages = [documentAssembler,\n",
        "                                      tokenizer,\n",
        "                                      lemma,\n",
        "                                      stopwords_cleaner,\n",
        "                                      bert_sent,\n",
        "                                      classsifierdl])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cR2f9uqPnewb"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "combined_pipeline_Model = combined_pipeline.fit(trainDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pU_KDbXnmPU"
      },
      "outputs": [],
      "source": [
        "preds = combined_pipeline_Model.transform(testDataset)\n",
        "\n",
        "preds_df = preds.select('category','description',\"class.result\").toPandas()\n",
        "\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "\n",
        "print (classification_report(preds_df['category'], preds_df['result']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR6tFGp6r0-z"
      },
      "source": [
        "#c, use RoBerta embeddings on top of best pipeline from b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRWTUhsPr-l3"
      },
      "outputs": [],
      "source": [
        "! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the command below ì roberta pipeline stuck at stopwords cleanẻ stage"
      ],
      "metadata": {
        "id": "nLvqoi6wTn_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.3.0 -s 4.3.2 -g"
      ],
      "metadata": {
        "id": "ls7JukklTk3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCt9EV2HBOWH"
      },
      "outputs": [],
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"description\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "    .setInputCols(\"token\")\\\n",
        "    .setOutputCol(\"cleanTokens\")\\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"lemma\")\n",
        "\n",
        "#roberta_embeddings = RoBertaEmbeddings.pretrained(\"roberta_embeddings_distilroberta_base\",\"en\") \\\n",
        "#    .setInputCols([\"document\", \"token\"]) \\\n",
        "#    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "roberta_embeddings = RoBertaEmbeddings.pretrained(\"roberta_base\", \"en\") \\\n",
        "    .setInputCols([\"document\", \"token\"]) \\\n",
        "    .setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "classsifierdl = ClassifierDLApproach()\\\n",
        "    .setInputCols([\"sentence_embeddings\"])\\\n",
        "    .setOutputCol(\"class\")\\\n",
        "    .setLabelColumn(\"category\")\\\n",
        "    .setMaxEpochs(5)\\\n",
        "    .setEnableOutputLogs(True)\\\n",
        "    .setLr(0.001)\\\n",
        "    .setBatchSize(1)\n",
        "\n",
        "roberta_pipeline = Pipeline(stages = [document,\n",
        "                                      tokenizer,\n",
        "                                      lemma,\n",
        "                                      stopwords_cleaner,\n",
        "                                      roberta_embeddings,\n",
        "                                      classsifierdl])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "roberta_pipeline_Model = roberta_pipeline.fit(trainDataset)"
      ],
      "metadata": {
        "id": "zzwWe6dnpb3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = roberta_pipeline_Model.transform(testDataset)\n",
        "\n",
        "preds_df = preds.select('category','description',\"class.result\").toPandas()\n",
        "\n",
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "\n",
        "print (classification_report(preds_df['category'], preds_df['result']))"
      ],
      "metadata": {
        "id": "iJzA-A2cpvm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LBZ2btOJqF1T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}